# Natural Language Processing 
 ## 100 Days of ML Coding Challenge Log ðŸ’¥
 Deep learning always fascinates me:smiley: Wanted to challenge myself again by starting 100DaysOfMLCode for the second time. This time I wish to dive deeper into the topics like NLP, computer vision. By the end, I hope to do really cool projects and contributions to the ML community
 
## Day 1 | Text Preprocessing 
Unlike in computer vision where we fed the DNN the pixel values, in NLP it's not possible to feed words or letters to the DNN. For the DNNs to learn about the corpus, some text preprocessing tasks are to be performed. So these are the topics I have learned
1. Tokenization
2.  Word Index
3. Text sequence
4. Padding  

## Day 2
Implemented text preprocessing on a real dataset for sarcasm detection. Check the [code](https://github.com/neha-duggirala/100DaysOfMLCode/blob/master/NLP/Sarcasm_detection.ipynb)  here. You can find the [DatasetðŸ“š](https://rishabhmisra.github.io/publications/)

## Day 3 | ML Project
Started working on Bosch Production Line dataset from Kaggle. It has 970 features, 1183747 records and data cleaning has to be performed.
Link to the [Dataset](https://www.kaggle.com/c/bosch-production-line-performance/overview)

## Day 4 | Word Embeddings
The idea that words and associated words are clustered as vectors in a multi-dimensional space is called Word Embedding.

## Day 5,6,7 | Projects

### Tensors
[Tensors](https://www.tensorflow.org/guide/tensor) are multi-dimensional arrays with a uniform type. They are immutable( cannot be changed) There are scalars, 1d, 2d and soon. A 4d tensor of shape (3,2,4,5) can be visualized as follows<br>
<img src='https://www.tensorflow.org/guide/images/tensor/4-axis_block.png' width=250 height=250/>
<img src='https://www.tensorflow.org/guide/images/tensor/shape2.png' width=250 height=250/>
