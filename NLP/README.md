# Natural Language Processing 
 ## 100 Days of ML Coding Challenge Log ðŸ’¥
 Deep learning always fascinates me:smiley: Wanted to challenge myself again by starting 100DaysOfMLCode for the second time. This time I wish to dive deeper into the topics like NLP, computer vision. By the end, I hope to do really cool projects and contributions to the ML community
 
## Day 1 | Text Preprocessing 
Unlike in computer vision where we fed the DNN the pixel values, in NLP it's not possible to feed words or letters to the DNN. For the DNNs to learn about the corpus, some text preprocessing tasks are to be performed. So these are the topics I have learned
1. Tokenization
2.  Word Index
3. Text sequence
4. Padding  

## Day 2
Implemented text preprocessing on a real dataset for sarcasm detection. Check the [code](https://github.com/neha-duggirala/100DaysOfMLCode/blob/master/NLP/Sarcasm_detection.ipynb)  here. You can find the [DatasetðŸ“š](https://rishabhmisra.github.io/publications/)

## Day 3 | ML Project
Started working on Bosch Production Line dataset from Kaggle. It has 970 features, 1183747 records and data cleaning has to be performed.
Link to the [Dataset](https://www.kaggle.com/c/bosch-production-line-performance/overview)

## Day 4 | Word Embeddings
The idea that words and associated words are clustered as vectors in a multi-dimensional space is called Word Embedding.

## Day 5,6,7 | Projects
There various topics I have covered.
### Tensors
[Tensors](https://www.tensorflow.org/guide/tensor) are multi-dimensional arrays with a uniform type. They are immutable( cannot be changed) There are scalars, 1d, 2d and soon. A 4d tensor of shape (3,2,4,5) can be visualized as follows<br>
<img src='https://www.tensorflow.org/guide/images/tensor/4-axis_block.png' width=250 height=250/>
<img src='https://www.tensorflow.org/guide/images/tensor/shape2.png' width=250 height=250/>

### How to use weights and bias
[Tutorials](https://app.wandb.ai)

### Global avg pooling vs flattening

## Day 8,9 | Theory

Watched a [playlist](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2&t=878s) about word vectors and origin of NLP

## Day 10,11 | Word2Vec
Understood the math for likelihood, cost function, prediction function and similarity between the center word vector and other word vector. This is very interesting [Video](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=1)
<img src="https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-composition.png?w=656"/>
[In Word2Vec Explained!](https://www.youtube.com/watch?v=yFFp9RYpOb0), 
